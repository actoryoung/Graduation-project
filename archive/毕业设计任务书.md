# 毕业设计任务书

---

## 学院（部）：计算机科学与技术学院
## 设计（论文）选题：**多模态情感分析方法研究与应用系统实现**

---

## 一、设计（论文）的主要任务及目标

### 目标

本项目旨在深入研究多模态情感分析的理论与方法，基于深度学习技术构建一个能够融合文本、语音、视频三种模态信息进行情感识别的系统，并在CMU-MOSEI公开数据集上验证模型性能。通过改进多模态融合策略和优化模型架构，提升情感识别的准确率，最终实现一个可用于实际场景的多模态情感分析原型系统。

### 主要任务

**(1)** 深入调研多模态情感分析领域的研究现状，系统梳理文本情感分析、语音情感识别、视频表情识别等单模态分析方法，以及多模态融合技术的发展脉络和最新进展；

**(2)** 研究并掌握深度学习在多模态学习中的应用，包括Transformer架构、注意力机制、预训练模型等关键技术，对比分析不同多模态融合策略（早期融合、晚期融合、混合融合）的优缺点；

**(3)** 设计并实现基于CMU-MOSEI数据集的多模态情感分析模型，包括：文本特征提取模块（GloVe/预训练语言模型）、音频特征提取模块（COVAREP/wav2vec）、视频特征提取模块（OpenFace/视觉编码器），以及跨模态注意力融合网络；

**(4)** 设计并进行系统的对比实验和消融实验，包括：单模态与多模态性能对比、不同融合策略效果对比、不同预训练编码器的效果分析、模型各组件贡献度分析等，验证改进方法的有效性；

**(5)** 开发一个简单的Web原型系统，能够接受用户输入的文本、音频或视频数据，实时输出情感分析结果并进行可视化展示，完成毕业论文的撰写。

---

## 二、设计（论文）的主要内容

**(1)** 文献调研与理论研究：系统阅读国内外多模态情感分析相关文献50篇以上（其中外文文献不少于15篇），重点研究基于注意力机制的多模态融合方法、预训练模型在情感分析中的应用、以及最新的多模态表示学习技术，撰写文献综述；

**(2)** 数据集处理与特征工程：使用CMU-MOSEI公开数据集，实现数据的预处理、清洗和标注，包括文本GloVe词向量提取、音频COVAREP特征提取、视频OpenFace面部特征提取，研究不同特征表示方法对模型性能的影响；

**(3)** 基线模型设计与实现：构建基于简单特征拼接融合的基线模型，实现文本编码器、音频编码器、视频编码器和情感分类器，在CMU-MOSEI数据集上训练并建立性能基线（目标测试准确率≥50%）；

**(4)** 改进模型设计：在基线模型基础上，设计并实现以下改进：
  - 跨模态注意力融合机制：实现文本、音频、视频之间的注意力交互
  - 预训练编码器集成：集成BERT（文本）、wav2vec 2.0（音频）等预训练模型
  - 门控融合策略：实现模态门控机制，动态调整不同模态的贡献权重
  - 对比不同改进方法的组合效果，确定最优模型架构；

**(5)** 实验设计与结果分析：设计完整的实验方案，包括对比实验（单模态vs多模态、不同融合策略）、消融实验（各组件贡献度分析）、参数敏感性分析等，使用准确率、精确率、召回率、F1分数等指标进行评估，绘制混淆矩阵、ROC曲线、训练曲线等可视化图表；

**(6)** 系统实现与集成：基于Python/PyTorch实现完整的多模态情感分析系统，包括后端推理API和前端Web界面（使用Streamlit或Flask），支持文本输入、音频/视频文件上传，实时返回情感分类结果和置信度；

**(7)** 论文撰写：按照学术规范撰写毕业论文，包括绪论、相关工作、方法设计、实验与结果分析、系统实现、总结与展望等章节，正文总字数1.2万-1.5万字。

---

## 三、设计（论文）的基本要求

### 1. 技术要求

**(1)** 熟练掌握Python编程语言和PyTorch深度学习框架，能够独立实现深度神经网络模型；

**(2)** 深入理解多模态学习的核心概念，包括特征提取、模态融合、注意力机制、预训练模型等，能够设计合理的多模态融合架构；

**(3)** 掌握自然语言处理、语音信号处理、计算机视觉等领域的特征提取方法，能够使用或调用相关预训练模型提取有效特征；

**(4)** 系统代码结构清晰、注释完整、具有良好的可读性和可维护性，遵循软件工程规范；

**(5)** 系统能够正常运行，在CMU-MOSEI测试集上达到52%以上的准确率基线，改进模型目标达到55%以上。

### 2. 实验要求

**(1)** 使用CMU-MOSEI标准数据集进行实验，采用官方的训练/验证/测试集划分；

**(2)** 设计完整的对比实验：
  - 单模态（文本、音频、视频）vs 多模态融合
  - 简单拼接 vs 注意力融合 vs 门控融合
  - 传统特征 vs 预训练编码器特征

**(3)** 设计消融实验：
  - 不同模态的贡献度分析
  - 注意力机制的效果验证
  - 融合策略的影响分析

**(4)** 实验结果真实可信，数据完整，所有实验需可复现，记录详细的超参数设置和训练过程。

### 3. 学术要求

**(1)** 论文结构完整，逻辑清晰，层次分明，符合学术规范；

**(2)** 文字表达准确、流畅，专业术语使用规范；

**(3)** 图表规范、清晰，编号和标题完整，所有图表需在正文中引用说明；

**(4)** 参考文献格式规范，采用标准学术引用格式，数量不少于35篇，其中外文文献不少于15篇，且需包含近3年的最新研究；

**(5)** 论文正文字数1.2万-1.5万字，需包含必要的理论分析、算法推导和实验讨论。

### 4. 规范要求

**(1)** 遵守学术道德，杜绝抄袭，所有引用需明确标注；

**(2)** 按时完成各阶段任务，每周向指导教师汇报进展；

**(3)** 代码版本管理规范，使用Git进行版本控制；

**(4)** 实验记录完整，保存所有实验结果和模型检查点。

---

## 四、主要参考文献

### 中文文献

[1] 周明. 多模态情感分析研究综述[J]. 计算机学报, 2020, 43(6): 1021-1040.

[2] 张华, 李明. 基于深度学习的多模态情感分析研究[J]. 软件学报, 2021, 32(3): 789-803.

[3] 王伟, 刘芳. 跨模态注意力机制在情感分析中的应用[J]. 自动化学报, 2021, 47(5): 1123-1135.

[4] 陈静, 吴强. 多模态融合方法综述[J]. 中国图象图形学报, 2020, 25(8): 1602-1620.

[5] 赵磊, 孙杰. 基于Transformer的多模态情感识别研究[J]. 计算机研究与发展, 2022, 59(2): 345-358.

[6] 李强, 张敏. BERT预训练模型在情感分析中的应用[J]. 计算机工程与应用, 2021, 57(15): 200-208.

[7] 吴昊, 周凯. 多模态情感分析数据集综述[J]. 数据分析与知识发现, 2022, 6(3): 22-35.

[8] 徐明, 王芳. 基于注意力机制的多模态数据融合方法[J]. 计算机应用, 2021, 41(6): 1650-1657.

[9] 胡晓, 林峰. 情感计算研究综述[J]. 软件学报, 2019, 30(1): 88-106.

[10] 谢明, 方圆. 多模态学习中的特征对齐方法[J]. 自动化学报, 2022, 48(1): 1-15.

[11] 刘洋, 马超. 语音情感识别技术综述[J]. 声学学报, 2020, 45(4): 512-525.

[12] 黄欣, 郑伟. 面部表情识别研究进展[J]. 中国图象图形学报, 2021, 26(2): 245-260.

[13] 杨帆, 李娜. 多模态深度学习综述[J]. 计算机科学, 2020, 47(9): 1-10.

[14] 郑涛, 何晶. 基于深度神经网络的情感分类方法研究[J]. 模式识别与人工智能, 2020, 33(4): 327-338.

[15] 林浩, 苏勇. 视频情感分析技术研究进展[J]. 计算机工程, 2021, 47(7): 1-8.

### 外文文献

[16] Poria S, Cambria E, Hazarika D, et al. Context-dependent sentiment analysis in user-generated videos[C]//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. 2017: 873-883.

[17] Tsai Y H H, Bai S, Yamada M, et al. Multimodal transformer for unaligned multimodal language sequences[C]//Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019: 6558-6567.

[18] Hazarika D, Poria S, Zadeh A, et al. Conversational memory network for emotion recognition in dyadic dialogue videos[C]//Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics. 2018: 1622-1631.

[19] Zadeh A, Liang P P, Mazumder N, et al. Memory fusion network for multi-view classification[C]//Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics. 2018: 780-789.

[20] Li Y, Ma H, Zhang X, et al. Multimodal affective analysis using hierarchical attention strategy with word-level alignment[C]//Proceedings of the 27th International Joint Conference on Artificial Intelligence. 2018: 4365-4371.

[21] Baltrušaitis T, Ahuja C, Morency L P. Multimodal machine learning: A survey and taxonomy[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 41(2): 423-443.

[22] Perez-Rosas V, Mihalcea R, Morency L P. Utterance-level multimodal sentiment analysis[C]//Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. 2013: 973-983.

[23] Xu H, Liu Y, Shu H, et al. Multi-view attention network for multilingual multimodal emotion recognition[C]//Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. 2020: 7492-7502.

[24] Wang Y, Shen L, Liu L, et al. Multi-modal sentiment classification with word-level and sentence-level attention fusion[C]//China National Conference on Chinese Computational Linguistics. Springer, 2019: 220-231.

[25] Devlin J, Chang M W, Lee K, et al. BERT: Pre-training of deep bidirectional transformers for language understanding[C]//Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. 2019: 4171-4186.

[26] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]//Advances in Neural Information Processing Systems. 2017: 5998-6008.

[27] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 770-778.

[28] Baevski A, Zhou Y, Mohamed A, et al. wav2vec 2.0: A framework for self-supervised learning of speech representations[C]//ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021: 6683-6687.

[29] Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[C]//International Conference on Learning Representations. 2021.

[30] Shen D, Chen M, Liu X, et al. Multimodal sentiment analysis using hierarchical fusion with context and bio-signals[C]//Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023: 12350-12366.

[31] Han J, Ding C, Zou Y, et al. Language models as zero-shot planners: Extracting multimodal reasoning for embodied visual navigation[C]//Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021: 2155-2167.

[32] Li S, Jin X, Xuan Z, et al. A unified pretraining framework for cross-modal understanding and generation[C]//Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021: 1560-1572.

[33] Liu Y, Ott M, Goyal N, et al. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension[C]//Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2021: 7971-7980.

[34] Chen S, Zhao S, Guan D, et al. Dynamic early fusion for multimodal data with different temporal dynamics[C]//Proceedings of the 30th ACM International Conference on Multimedia. 2021: 2505-2514.

[35] Hsu C C, Huang S, Ku L. MISA: Modality-invariant and specific representation for multimedia sentiment analysis[C]//Proceedings of the 29th ACM International Conference on Multimedia. 2021: 1121-1129.

---

## 五、进度安排

| 阶段 | 时间 | 工作内容 |
|------|------|----------|
| **第一阶段** | **第1-2周** | 查阅相关资料，收集和阅读多模态情感分析相关文献，了解研究现状和发展趋势，确定具体研究方向和技术路线 |
| **第二阶段** | **第3-4周** | 完成文献综述撰写（1000字以上），完成外文文献翻译（3000单词以上），搭建开发环境和数据处理流程 |
| **第三阶段** | **第5-7周** | 实现数据预处理和基线模型，包括特征提取模块、简单融合网络、训练流程，在CMU-MOSEI数据集上建立性能基线（目标≥50%） |
| **第四阶段** | **第8-11周** | 设计并实现改进模型，包括跨模态注意力融合、预训练编码器集成、门控融合策略等，进行对比实验和消融实验，优化模型性能 |
| **第五阶段** | **第12-13周** | 开发Web原型系统，实现后端推理API和前端界面，完成系统集成和功能测试，撰写毕业论文初稿 |
| **第六阶段** | **第14周** | 完善系统功能，整理实验数据和可视化图表，修改并完善毕业论文，准备中期检查材料 |
| **第七阶段** | **第15周** | 论文定稿，准备答辩PPT和系统演示材料，进行答辩预演，参加毕业设计答辩 |

---

**主要技术指标：**

1. **数据集**：CMU-MOSEI标准数据集（训练集2249样本、验证集300样本、测试集676样本）

2. **基线模型性能**：
   - 测试集准确率 ≥ 52%
   - 7类情感分类（强烈负面到强烈正面）

3. **改进模型目标**：
   - 测试集准确率 ≥ 55%
   - 相比基线提升至少3个百分点
   - F1分数 ≥ 0.50

4. **系统功能**：
   - 支持文本、音频、视频三种模态输入
   - 实时推理响应时间 < 3秒
   - Web界面可用，支持文件上传和结果可视化

5. **文档要求**：
   - 毕业论文正文1.2万-1.5万字
   - 参考文献≥35篇（外文≥15篇）
   - 文献综述≥1000字
   - 外文翻译≥3000单词

---

## 关键时间节点

| 时间节点 | 任务 | 要求 |
|----------|------|------|
| 第4周末 | 文献综述、外文翻译上传 | 文献综述1000+字，外文翻译3000+单词 |
| 第7周末 | 基线模型完成 | 准确率≥50%，建立性能基线 |
| 第11周末 | 改进模型完成 | 准确率≥55%，完成实验分析 |
| 第12周末 | 中期检查 | 系统原型基本完成，通过检查 |
| 第13周末 | 论文初稿完成 | 论文结构完整，实验数据充实 |
| 第15周末 | 毕业答辩 | 论文定稿，准备充分，顺利答辩 |

---

## 指导教师意见

（此处由指导教师填写）

---

## 系/教研室审核意见

（此处由系/教研室填写）

---

**任务书制定日期**：2025年2月

**计划完成时间**：2026年5月

---

*注：本任务书一式三份，学院（部）、指导教师、学生各执一份。*
