# 论文引用标记添加指南

**文档目的**: 标注各章节需要添加图表引用和文献引用的具体位置
**创建日期**: 2026-02-09

---

## 📋 图表引用对照表

| 图表 | 文件名 | 引用位置 |
|------|--------|----------|
| **图1** | fig1_label_distribution.png | 1.1.3节（类别不平衡问题）、3.1.2节（SDK子集特性） |
| **图2** | fig2_model_progression.png | 5.4.4节（与基线模型对比）、4.4.4节（实验结果） |
| **图3** | fig3_s10_comparison.png | 4.4.4节（S10实验结果）、5.4.4节（性能对比） |
| **图4** | fig4_confusion_matrix.png | 5.4.2节（混淆矩阵分析） |
| **图5** | fig5_performance_radar.png | 5.4.3节（各类别性能对比） |
| **图6** | fig6_dataset_size_comparison.png | 3.1.2节（数据集规模）、2.1.2节（CMU-MOSEI介绍） |
| **图7** | fig7_threshold_sensitivity.png | 4.4.1节（阈值优化） |
| **图8** | fig8_training_curves.png | 4.4.3节（S10-3 CE训练） |

---

## 📚 文献引用对照表

### 按章节分类

#### 第一章 绪论

| 文献 | 引用内容 | 引用位置 |
|------|---------|----------|
| [1] [2] | 社交媒体用户数据 | 1.1.1节 |
| [3] | 心理学7%-38%-55%研究 | 1.1.1节 |
| [4] | CMU-MOSEI数据集介绍 | 1.3节 |
| [5] | 多模态融合基础方法 | 1.2.1节 |
| [10] | 注意力机制 | 1.2.2节 |
| [12] [13] [14] | 类别不平衡方法 | 1.2.3节 |

#### 第二章 相关技术

| 文献 | 引用内容 | 引用位置 |
|------|---------|----------|
| [4] | CMU-MOSEI详细介绍 | 2.1.2节 |
| [10] | Transformer注意力 | 2.2.1节 |
| [6] [7] | TFN/LMF融合方法 | 2.2.2节 |
| [8] [9] | MISA/Mulan模型 | 2.2.2节 |
| [11] | 多头注意力应用 | 2.2.1节 |
| [12] | SMOTE过采样 | 2.3.1节 |
| [13] | Focal Loss | 2.3.1节 |
| [14] | 代价敏感学习 | 2.3.1节 |
| [16] | GloVe词向量 | 2.1.2节 |

#### 第三章 系统设计

| 文献 | 引用内容 | 引用位置 |
|------|---------|----------|
| [4] | SDK子集特性 | 3.1.2节 |
| [16] | GloVe特征维度 | 3.2.2节 |
| [17] | COVAREP特征 | 3.2.2节 |
| [18] | OpenFace特征 | 3.2.2节 |

#### 第四章 关键技术实现

| 文献 | 引用内容 | 引用位置 |
|------|---------|----------|
| [16] | GloVe实现细节 | 4.1.1节 |
| [17] | COVAREP特征说明 | 4.1.2节 |
| [18] | OpenFace AU说明 | 4.1.3节 |
| [10] | 注意力公式 | 4.2.2节 |
| [13] | Focal Loss公式 | 4.4.2节 |

#### 第五章 系统实现与测试

| 文献 | 引用内容 | 引用位置 |
|------|---------|----------|
| [4] | 测试集划分 | 5.1.1节 |
| [16] [17] [18] | 特征工具 | 5.1.2节 |

#### 第六章 总结与展望

| 文献 | 引用内容 | 引用位置 |
|------|---------|----------|
| [4] | CMU-MOSEI数据集 | 6.2.1节 |
| [10] | 注意力机制创新 | 6.2.2节 |

---

## 📝 各章节具体引用位置

### 第一章（绪论）

#### 1.1.3 类别不平衡问题的普遍性

**添加图表引用**：
> "以CMU-MOSEI数据集的SDK标准子集为例，本研究发现该子集的标签分布存在严重不平衡：Neutral（中性）情感占50.4%，Positive（积极）占39.6%，而Negative（消极）仅占10.0%【**图1**】。"

**添加文献引用**：
> "心理学研究表明，在面对面的情感交流中，语言内容仅传递7%的信息，而语调传递38%，面部表情和肢体语言传递55%[3]。"

#### 1.2.1 多模态融合策略研究

**添加文献引用**：
> "早期融合（Early Fusion）在特征层面直接拼接多模态特征。Poria等人[5]最早使用早期融合方法进行多模态情感分析。"

> "张量融合网络（TFN）通过建模模态间的三向交互提升性能[6]。"

#### 1.2.2 注意力机制在情感分析中的应用

**添加文献引用**：
> "注意力机制（Attention Mechanism）最初由Vaswani等人[10]在Transformer模型中提出。"

> "Chen等人[11]提出使用多头注意力进行多模态融合，在CMU-MOSEI数据集上取得了当时最优的性能。"

#### 1.2.3 类别不平衡处理方法

**添加文献引用**：
> "Focal Loss[13]是代表性的方法，它在标准交叉熵的基础上引入调制因子。"

> "代价敏感学习（Cost-sensitive Learning）[14]通过为不同类别分配不同代价来平衡学习。"

---

### 第二章（相关技术）

#### 2.1.2 CMU-MOSEI数据集介绍

**添加图表引用**：
> "SDK子集与完整数据集的规模对比如【**图6**】所示，SDK子集约为完整数据集的10%。"

**添加文献引用**：
> "CMU-MOSEI（Multimodal Opinion Sentiment and Emotion Intensity）[4]是当前多模态情感分析领域规模最大、使用最广泛的数据集之一。"

#### 2.2.1 注意力机制

**添加文献引用**：
> "缩放点积注意力（Scaled Dot-Product Attention）的计算如下[10]："

$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

#### 2.2.2 多模态融合策略

**添加文献引用**：
> "张量融合网络（TFN）[6]通过张量外积建模模态间的三向交互："

$$ h = \text{Fuse}(h_T, h_A, h_V) = h_T \otimes h_A \otimes h_V $$

#### 2.3.1 损失函数改进方法

**添加文献引用**：
> "Focal Loss[13]在标准交叉熵的基础上引入调制因子："

$$ \text{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t) $$

#### 2.3.2 类别权重策略

**添加文献引用**：
> "类别加权交叉熵（Class-weighted Cross Entropy）为不同类别分配不同权重[14]。"

---

### 第三章（系统设计）

#### 3.1.2 总体架构设计

**添加图表引用**：
> "系统采用分层模块化架构【**图6**】所示，从下到上分为四层：数据输入层、特征提取层、融合推理层和结果展示层。"

#### 3.2.2 特征提取模块设计

**添加文献引用**：
> "GloVe（Global Vectors for Word Representation）词向量[16]对输入文本进行编码。"

> "COVAREP（Coherent and Extracted Representation of Audio）工具提取74维声学特征[17]。"

> "OpenFace工具提取710维面部特征[18]。"

---

### 第四章（关键技术实现）

#### 4.1.1 文本特征提取（GloVe）

**添加文献引用**：
> "GloVe由Pennington等人[16]于2014年提出，是一种基于全局词共现统计的无监督学习方法。"

> "本研究使用在Common Crawl数据集（840亿词token）上预训练的GloVe模型（glove.6B.300d）。"

#### 4.2.2 跨模态注意力设计

**添加文献引用**：
> "注意力机制最初由Vaswani等人[10]在2017年提出，用于机器翻译任务。"

> "本研究设计的跨模态注意力融合机制包含三个步骤【**图8**】。"

#### 4.4.3 最终方案设计与实现

**添加图表引用**：
> "S10优化方案的实验结果如【**图3**】所示，S10-3 CE训练模型在验证集第10个epoch达到最优Macro F1值。"

---

### 第五章（系统实现与测试）

#### 5.4.2 混淆矩阵分析

**添加图表引用**：
> "S10-3模型的混淆矩阵如【**图4**】所示，展示了预测结果与真实标签的对应关系。"

#### 5.4.3 各类别性能对比

**添加图表引用**：
> "各类别详细性能对比如【**图5**】所示，通过雷达图直观对比S3和S10-3模型在各类别上的表现。"

#### 5.4.4 与基线模型对比

**添加图表引用**：
> "模型性能演变路径如【**图2**】所示，从基线到最终模型的完整演变过程清晰可见。"

> "各基线模型的详细对比如【**图2**】和【**表5-1**】所示。"

---

### 第六章（总结与展望）

#### 6.2.1 SDK子集数据特性发现与验证

**添加图表引用**：
> "SDK子集与完整数据集的规模对比如【**图6**】所示，SDK子集约为完整数据集的10%。"

**添加文献引用**：
> "本研究首次系统分析了CMU-MOSEI SDK子集的数据特性，发现其与完整数据集在规模（约1:10）和标签分布（50.4% vs 26.1%中性情感）上存在显著差异[4]。"

#### 6.2.2 注意力融合与类别权重结合方案

**添加文献引用**：
> "注意力机制（Attention Mechanism）最初由Vaswani等人[10]提出，现已成为深度学习的基础组件。"

#### 6.3.2 未来改进方向

**添加文献引用**：
> "未来可以使用wav2vec 2.0或HuBERT等预训练模型[20-21]提取音频特征。"

---

## 🔧 引用格式规范

### 图表引用格式

**单图引用**：
> 如图1所示，SDK子集的标签分布与完整数据集存在显著差异。

**多图引用**：
> 如图1和图6所示，SDK子集在规模和分布上都与完整数据集存在差异。

**图表子图引用**：
> 如图4(a)所示，原始混淆矩阵显示各类别的预测数量。
> 如图4(b)所示，归一化混淆矩阵展示召回率百分比。

### 文献引用格式

**单文献引用**：
> 注意力机制（Attention Mechanism）最初由Vaswani等人[10]在2017年提出。

**多文献引用**：
> Focal Loss[13]和代价敏感学习[14]是处理类别不平衡问题的两种主要方法。

**连续文献引用**：
> 相关研究包括TFN[6]、LMF[7]、MISA[8]和Mulan[9]等多模态融合方法。

### 引用编号规则

- **顺序编码**：按在文中首次出现的顺序编号
- **同一处多文献**：用方括号括起来，如[1,2,3]
- **连续编号**：如[1-3]表示1,2,3
- **引用位置**：
  - 放在引用内容之后，如"数据集[4]"
  - 放在句子末尾，如"...用于情感分析[5][6]"
  - 放在专有名词之后，如"Transformer模型[10]"

---

## 📋 快速引用插入列表

### 需要添加的图表引用汇总

```
第一章：
- [1.1.3] 图1 - 标签分布
- [1.3节] 图6 - 数据集规模

第二章：
- [2.1.2] 图6 - 数据集规模
- [2.2.1] 无（公式引用文献）
- [2.2.2] 无（方法引用文献）

第三章：
- [3.1.2] 图6 - 架构/规模
- [3.2.2] 无（工具引用文献）

第四章：
- [4.2.2] 图8 - 注意力机制
- [4.3] 无（公式引用文献）
- [4.4.1] 图7 - 阈值优化
- [4.4.3] 图8 - 训练曲线
- [4.4.4] 图3 - S10对比

第五章：
- [5.4.2] 图4 - 混淆矩阵
- [5.4.3] 图5 - 雷达图
- [5.4.4] 图2 - 模型对比

第六章：
- [6.2.1] 图6 - 数据集规模
```

### 需要添加的文献引用汇总

```
高频文献（出现5次以上）：
- [4] CMU-MOSEI数据集
- [10] Transformer注意力
- [13] Focal Loss

中频文献（出现2-4次）：
- [3] 心理学研究
- [6] TFN
- [7] LMF
- [16] GloVe
- [17] COVAREP
- [18] OpenFace

低频文献（出现1次）：
- [1] YouTube统计
- [2] TikTok统计
- [5] Poria等人的工作
- [8] MISA
- [9] Mulan
- [11] Chen等人的工作
- [12] SMOTE
- [14] 代价敏感学习
- [15] 在线难样本挖掘
```

---

## ⚠️ 注意事项

1. **图表编号**：使用中文"图1"而非英文"Figure 1"
2. **图表编号与文件对应**：确保编号与fig1, fig2等文件名一致
3. **文献编号**：使用方括号，如[4]、[10]
4. **引用位置**：一般放在引用内容之后，或句子末尾
5. **避免过度引用**：同一内容不需要反复引用同一文献

---

## 📝 完成检查清单

添加引用后，请检查：

- [ ] 所有图表都有对应引用
- [ ] 所有重要文献都有引用
- [ ] 引用编号连续且无重复
- [ ] 引用格式一致（都是[x]格式）
- [ ] 图表文件名与引用编号一致
- [ ] 文献列表中的编号与文中引用一致

---

**创建时间**: 2026-02-09
**版本**: v1.0
**状态**: 待执行
