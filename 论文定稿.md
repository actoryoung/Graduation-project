# 多模态情感分析系统设计与实现
**——基于CMU-MOSEI数据集的类别不平衡优化研究**

---

## 摘要

随着社交媒体的快速发展，多模态情感分析已成为人机交互和舆情监控的重要技术手段。情感表达本质上是多模态的：文本传达语义内容，音频携带语气和情感韵律，视觉包含面部表情和肢体语言。然而，实际应用中面临两个关键挑战：一是如何有效融合异构多模态特征；二是如何处理数据集中普遍存在的类别不平衡问题。

本文在CMU-MOSEI数据集的SDK标准子集上设计并实现了一个多模态情感分析系统。系统采用多头跨模态注意力机制整合文本、音频、视频三种模态信息，并通过系统的优化方案解决类别不平衡问题。具体而言，本文首先使用GloVe（300维）、COVAREP（74维）和OpenFace（710维）分别提取三种模态的特征表示，并通过各自的MLP编码器投影到统一的64维空间。其次，设计多头跨模态注意力模块，将三种模态作为序列输入，通过自注意力学习模态间的动态交互关系。最后，针对SDK子集严重的类别不平衡问题（Negative占10%，Neutral占50.4%），提出包含阈值优化和损失函数调整的系统性解决方案。

本文的主要工作和贡献如下：（1）SDK子集数据特性分析与验证。研究发现SDK子集规模约为完整数据集的10%（2,249 vs 22,834训练样本），标签分布存在显著差异（Neutral 50.4% vs 26.1%）。实验发现，预训练模型（BERT）在此规模数据上表现不如简单特征（GloVe），揭示了数据特性对模型选择的影响。（2）多头跨模态注意力融合机制设计。实验表明，注意力融合相比简单特征拼接提升1.63%的准确率（59.17% vs 57.54%），验证了动态模态交互的有效性。（3）类别权重策略。针对Negative类完全失效问题（F1=0），通过平方根加权损失函数，将Negative F1恢复到25.97%，但整体准确率下降到56.80%，揭示了准确率与类别平衡之间的权衡关系。（4）系统性类别不平衡解决方案。通过阈值优化和损失函数调整的组合策略，最终模型（S10-3 CE）将Macro F1从0.4133提升到0.5011（+21.3%），Negative类F1从0.00%提升到27.59%，同时保持58.88%的整体准确率。

实验结果表明，本文方法在保持整体性能的同时显著改善了类别平衡。最终模型在678个测试样本上达到58.88%的准确率，Macro F1分数达到0.5011，超过0.50的目标。同时，本文基于Streamlit框架实现了完整的Web演示系统，支持文本输入、实时分析和结果可视化，验证了方法的实用性。

本研究为多模态情感分析中的类别不平衡问题提供了系统的解决思路，对类似的数据集和应用场景具有参考价值。未来的工作将重点改进Negative类识别能力，实现端到端特征提取，并扩展系统功能以支持实际应用部署。

**关键词**：多模态情感分析；类别不平衡；注意力机制；跨模态融合；CMU-MOSEI

---

# Abstract

With the rapid development of social media, multimodal sentiment analysis has become an important technology for human-computer interaction and public opinion monitoring. Emotional expression is inherently multimodal: text conveys semantic content, audio carries tone and emotional prosody, and vision contains facial expressions and body language. However, practical applications face two key challenges: how to effectively fuse heterogeneous multimodal features, and how to handle the class imbalance problem prevalent in datasets.

This paper designs and implements a multimodal sentiment analysis system on the SDK standard subset of the CMU-MOSEI dataset. The system employs a multi-head cross-modal attention mechanism to integrate information from text, audio, and video modalities, and addresses the class imbalance problem through a systematic optimization approach. Specifically, this paper first uses GloVe (300D), COVAREP (74D), and OpenFace (710D) to extract feature representations from three modalities, respectively, and projects them into a unified 64D space through respective MLP encoders. Second, a multi-head cross-modal attention module is designed, taking three modalities as sequence input and learning dynamic inter-modal interactions through self-attention. Finally, addressing the severe class imbalance in the SDK subset (Negative 10%, Neutral 50.4%), a systematic solution combining threshold optimization and loss function adjustment is proposed.

The main contributions of this paper are as follows: (1) Analysis and validation of SDK subset characteristics. The study found that the SDK subset is approximately 10% the size of the full dataset (2,249 vs 22,834 training samples), with significant differences in label distribution (Neutral 50.4% vs 26.1%). Experiments revealed that pre-trained models (BERT) perform worse than simple features (GloVe) on this scale of data, revealing the impact of data characteristics on model selection. (2) Design of multi-head cross-modal attention fusion mechanism. Experiments show that attention fusion improves accuracy by 1.63% compared to simple feature concatenation (59.17% vs 57.54%), validating the effectiveness of dynamic modal interaction. (3) Class weighting strategy. Addressing the complete failure of Negative class (F1=0), square-root weighted loss function restored Negative F1 to 25.97%, but overall accuracy dropped to 56.80%, revealing the trade-off between accuracy and class balance. (4) Systematic class imbalance solution. Through a combination strategy of threshold optimization and loss function adjustment, the final model (S10-3 CE) improved Macro F1 from 0.4133 to 0.5011 (+21.3%), Negative class F1 from 0.00% to 27.59%, while maintaining 58.88% overall accuracy.

Experimental results show that the proposed method significantly improves class balance while maintaining overall performance. The final model achieves 58.88% accuracy on 678 test samples, with a Macro F1 score of 0.5011, exceeding the 0.50 target. Additionally, a complete web demo system was implemented using the Streamlit framework, supporting text input, real-time analysis, and result visualization, validating the practicality of the method.

This research provides a systematic approach to address class imbalance in multimodal sentiment analysis, offering valuable insights for similar datasets and application scenarios. Future work will focus on improving Negative class recognition, implementing end-to-end feature extraction, and extending system functionality for practical deployment.

**Keywords**: Multimodal Sentiment Analysis; Class Imbalance; Attention Mechanism; Cross-modal Fusion; CMU-MOSEI

---

**[以下为正文内容]**

（注：为避免文件过长，以下是章节导航，完整内容请参考各章节文件）

---

## 目录

### 摘要
### Abstract

### 第一章 绪论
- 1.1 研究背景与意义
  - 1.1.1 多模态情感分析的应用需求
  - 1.1.2 社交媒体情感识别的挑战
  - 1.1.3 类别不平衡问题的普遍性
- 1.2 国内外研究现状
  - 1.2.1 多模态融合策略研究
  - 1.2.2 注意力机制在情感分析中的应用
  - 1.2.3 类别不平衡处理方法
- 1.3 本文主要工作与贡献
- 1.4 论文组织结构

### 第二章 相关技术
- 2.1 多模态情感分析
  - 2.1.1 多模态学习的定义与挑战
  - 2.1.2 CMU-MOSEI数据集介绍
  - 2.1.3 情感分析任务定义
- 2.2 深度学习基础技术
  - 2.2.1 注意力机制
  - 2.2.2 多模态融合策略
  - 2.2.3 集成学习方法
- 2.3 类别不平衡处理
  - 2.3.1 损失函数改进方法
  - 2.3.2 类别权重策略
  - 2.3.3 阈值优化技术

### 第三章 系统设计
- 3.1 系统总体架构
  - 3.1.1 系统功能需求分析
  - 3.1.2 总体架构设计
  - 3.1.3 模块划分
- 3.2 数据流设计
  - 3.2.1 输入模块设计
  - 3.2.2 特征提取模块设计
  - 3.2.3 融合推理模块设计
  - 3.2.4 输出展示模块设计
- 3.3 界面设计
  - 3.3.1 主界面布局
  - 3.3.2 交互流程设计
  - 3.3.3 可视化设计

### 第四章 关键技术实现
- 4.1 多模态特征提取
  - 4.1.1 文本特征提取（GloVe）
  - 4.1.2 音频特征提取（COVAREP）
  - 4.1.3 视频特征提取（OpenFace）
- 4.2 注意力融合机制
  - 4.2.1 问题背景与动机
  - 4.2.2 跨模态注意力设计
  - 4.2.3 实现细节
- 4.3 类别权重策略
  - 4.3.1 问题分析：Negative类失效
  - 4.3.2 权重计算方法
  - 4.3.3 加权损失函数实现
- 4.4 系统性优化方案
  - 4.4.1 优化思路：阈值调整
  - 4.4.2 优化思路：损失函数对比
  - 4.4.3 最终方案设计与实现
  - 4.4.4 实验结果与分析

### 第五章 系统实现与测试
- 5.1 开发环境与工具
  - 5.1.1 硬件与软件环境
  - 5.1.2 开发框架与库
- 5.2 系统界面展示
  - 5.2.1 主界面
  - 5.2.2 输入界面
  - 5.2.3 结果展示
- 5.3 功能测试
  - 5.3.1 单模态测试
  - 5.3.2 多模态测试
- 5.4 性能测试
  - 5.4.1 评估指标体系
  - 5.4.2 混淆矩阵分析
  - 5.4.3 各类别性能对比
  - 5.4.4 与基线模型对比

### 第六章 总结与展望
- 6.1 工作总结
  - 6.1.1 系统设计与实现总结
  - 6.1.2 关键技术创新总结
- 6.2 主要创新点
  - 6.2.1 SDK子集数据特性发现与验证
  - 6.2.2 注意力融合与类别权重结合方案
  - 6.2.3 系统性类别不平衡解决方案
- 6.3 不足与展望
  - 6.3.1 当前局限性
  - 6.3.2 未来改进方向

### 参考文献

### 附录
- 附录A：核心代码
- 附录B：完整实验数据
- 附录C：部署指南

---

**[论文定稿说明]**

本文档为论文的完整目录和摘要部分，约1,500字。各章节的详细内容请参考以下文件：

- 摘要完整内容：`论文章节初稿_摘要.md`
- 第一章：`论文章节初稿_第一章.md` (约4,500字)
- 第二章：`论文章节初稿_第二章.md` (约6,500字)
- 第三章：`论文章节初稿_第三章.md` (约5,500字)
- 第四章：`论文章节初稿_第四章.md` (约7,500字)
- 第五章：`论文章节初稿_第五章.md` (约5,000字)
- 第六章：`论文章节初稿_第六章.md` (约3,500字)

**论文总字数**：约34,000字（不含摘要）

**如需合并为单一Word/PDF文档，请告知，我可以提供合并脚本。**

---

**[论文元信息]**

| 项目 | 内容 |
|------|------|
| **论文题目** | 多模态情感分析系统设计与实现——基于CMU-MOSEI数据集的类别不平衡优化研究 |
| **作者** | [您的姓名] |
| **指导教师** | [导师姓名] |
| **学校** | [学校名称] |
| **专业** | [专业名称] |
| **完成时间** | 2026年2月 |
| **总字数** | 约34,000字 |
| **关键词** | 多模态情感分析；类别不平衡；注意力机制；跨模态融合；CMU-MOSEI |

---

**论文撰写状态**：✅ 第三阶段完成（章节初稿）

**待完成工作**：
- [ ] 生成图表和可视化内容
- [ ] 整理参考文献列表
- [ ] 添加图表引用标记和文献引用标记
- [ ] 格式调整和最终润色
