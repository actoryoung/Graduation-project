# 摘要

**摘要**

随着社交媒体的快速发展，多模态情感分析已成为人机交互和舆情监控的重要技术手段。情感表达本质上是多模态的：文本传达语义内容，音频携带语气和情感韵律，视觉包含面部表情和肢体语言。然而，实际应用中面临两个关键挑战：一是如何有效融合异构多模态特征；二是如何处理数据集中普遍存在的类别不平衡问题。

本文在CMU-MOSEI数据集的SDK标准子集上设计并实现了一个多模态情感分析系统。系统采用多头跨模态注意力机制整合文本、音频、视频三种模态信息，并通过系统的优化方案解决类别不平衡问题。具体而言，本文首先使用GloVe（300维）、COVAREP（74维）和OpenFace（710维）分别提取三种模态的特征表示，并通过各自的MLP编码器投影到统一的64维空间。其次，设计多头跨模态注意力模块，将三种模态作为序列输入，通过自注意力学习模态间的动态交互关系。最后，针对SDK子集严重的类别不平衡问题（Negative占10%，Neutral占50.4%），提出包含阈值优化和损失函数调整的系统性解决方案。

本文的主要工作和贡献如下：（1）SDK子集数据特性分析与验证。研究发现SDK子集规模约为完整数据集的10%（2,249 vs 22,834训练样本），标签分布存在显著差异（Neutral 50.4% vs 26.1%）。实验发现，预训练模型（BERT）在此规模数据上表现不如简单特征（GloVe），揭示了数据特性对模型选择的影响。（2）多头跨模态注意力融合机制设计。实验表明，注意力融合相比简单特征拼接提升1.63%的准确率（59.17% vs 57.54%），验证了动态模态交互的有效性。（3）类别权重策略。针对Negative类完全失效问题（F1=0），通过平方根加权损失函数，将Negative F1恢复到25.97%，但整体准确率下降到56.80%，揭示了准确率与类别平衡之间的权衡关系。（4）系统性类别不平衡解决方案。通过阈值优化和损失函数调整的组合策略，最终模型（S10-3 CE）将Macro F1从0.4133提升到0.5011（+21.3%），Negative类F1从0.00%提升到27.59%，同时保持58.88%的整体准确率。

实验结果表明，本文方法在保持整体性能的同时显著改善了类别平衡。最终模型在678个测试样本上达到58.88%的准确率，Macro F1分数达到0.5011，超过0.50的目标。同时，本文基于Streamlit框架实现了完整的Web演示系统，支持文本输入、实时分析和结果可视化，验证了方法的实用性。

本研究为多模态情感分析中的类别不平衡问题提供了系统的解决思路，对类似的数据集和应用场景具有参考价值。未来的工作将重点改进Negative类识别能力，实现端到端特征提取，并扩展系统功能以支持实际应用部署。

**关键词**：多模态情感分析；类别不平衡；注意力机制；跨模态融合；CMU-MOSEI

---

# Abstract

**Abstract**

With the rapid development of social media, multimodal sentiment analysis has become an important technology for human-computer interaction and public opinion monitoring. Emotional expression is inherently multimodal: text conveys semantic content, audio carries tone and emotional prosody, and vision contains facial expressions and body language. However, practical applications face two key challenges: how to effectively fuse heterogeneous multimodal features, and how to handle the class imbalance problem prevalent in datasets.

This paper designs and implements a multimodal sentiment analysis system on the SDK standard subset of the CMU-MOSEI dataset. The system employs a multi-head cross-modal attention mechanism to integrate information from text, audio, and video modalities, and addresses the class imbalance problem through a systematic optimization approach. Specifically, this paper first uses GloVe (300D), COVAREP (74D), and OpenFace (710D) to extract feature representations from three modalities, respectively, and projects them into a unified 64D space through respective MLP encoders. Second, a multi-head cross-modal attention module is designed, taking three modalities as sequence input and learning dynamic inter-modal interactions through self-attention. Finally, addressing the severe class imbalance in the SDK subset (Negative 10%, Neutral 50.4%), a systematic solution combining threshold optimization and loss function adjustment is proposed.

The main contributions of this paper are as follows: (1) Analysis and validation of SDK subset characteristics. The study found that the SDK subset is approximately 10% the size of the full dataset (2,249 vs 22,834 training samples), with significant differences in label distribution (Neutral 50.4% vs 26.1%). Experiments revealed that pre-trained models (BERT) perform worse than simple features (GloVe) on this scale of data, revealing the impact of data characteristics on model selection. (2) Design of multi-head cross-modal attention fusion mechanism. Experiments show that attention fusion improves accuracy by 1.63% compared to simple feature concatenation (59.17% vs 57.54%), validating the effectiveness of dynamic modal interaction. (3) Class weighting strategy. Addressing the complete failure of Negative class (F1=0), square-root weighted loss function restored Negative F1 to 25.97%, but overall accuracy dropped to 56.80%, revealing the trade-off between accuracy and class balance. (4) Systematic class imbalance solution. Through a combination strategy of threshold optimization and loss function adjustment, the final model (S10-3 CE) improved Macro F1 from 0.4133 to 0.5011 (+21.3%), Negative class F1 from 0.00% to 27.59%, while maintaining 58.88% overall accuracy.

Experimental results show that the proposed method significantly improves class balance while maintaining overall performance. The final model achieves 58.88% accuracy on 678 test samples, with a Macro F1 score of 0.5011, exceeding the 0.50 target. Additionally, a complete web demo system was implemented using the Streamlit framework, supporting text input, real-time analysis, and result visualization, validating the practicality of the method.

This research provides a systematic approach to address class imbalance in multimodal sentiment analysis, offering valuable insights for similar datasets and application scenarios. Future work will focus on improving Negative class recognition, implementing end-to-end feature extraction, and extending system functionality for practical deployment.

**Keywords**: Multimodal Sentiment Analysis; Class Imbalance; Attention Mechanism; Cross-modal Fusion; CMU-MOSEI
